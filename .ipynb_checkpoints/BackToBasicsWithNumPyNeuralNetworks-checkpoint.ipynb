{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcb4df8",
   "metadata": {},
   "source": [
    "Now we are going to code up a Nn from scratch, that means we are not going to use PyTorch instead we'll be using pure python and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = sklearn.datasets.make_moons(200, noise=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68636d95",
   "metadata": {},
   "source": [
    "Note: if we press Tab after writing this ***sklearn.datasets.*** , then we'll see all the ready made datasets in the sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3455cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908f9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the data\n",
    "plt.scatter(x[:,0],x[:,1],s=20,c=y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c44b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons = 2\n",
    "output_neurons = 2\n",
    "samples = x.shape[0]\n",
    "learning_rate = 0.001\n",
    "# Here, we'll use regularization from scratch as well.\n",
    "lamb = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6085837",
   "metadata": {},
   "source": [
    "#### This is the Neural Network that we are gonna build with 2 input neurons, 3 hidden neurons and 2 output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46065271",
   "metadata": {},
   "source": [
    "![NN Architecture](NN_from_scratch_architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78152ec8",
   "metadata": {},
   "source": [
    "In summary, our neural network has the following characteristics:\n",
    "\n",
    "**Architecture:** Two input neurons, three hidden neurons, and two output neurons.\n",
    "\n",
    "**Weight Updates:** we're planning to update the weights simultaneously using vectorized operations(Instead of updating weights one at a time, we use matrices to perform operations on multiple weights simultaneously.\n",
    "This means we calculate the gradients for all weights together, which is much faster).\n",
    "\n",
    "**Matrix Multiplication:** we'll use matrices to represent the three terms of the chain rule (derivative of error with respect to output, output with respect to input, and input with respect to weight).\n",
    "\n",
    "**Gradient Calculation:** Multiplying these matrices together will yield the gradient of the error with respect to the weights.\n",
    "\n",
    "**Weight Matrix:** Weights will also be represented as matrices.\n",
    "\n",
    "**Weight Update:** To update the weights, we'll subtract the gradients from the weight matrices.\n",
    "\n",
    "This approach allows for efficient computations and is commonly used when implementing neural networks using libraries like NumPy in Python. It helps optimize the training process and allows for parallelism when working with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45de10",
   "metadata": {},
   "source": [
    "![NN Architecture2](NN_from_scratch_architecture_pic2.png)\n",
    "\n",
    "This is what we have for first layer.\n",
    "shape of the input matrix is (200 * 2), 200 samples and each sample has 2 features and for the weight matrix the number of rows is the number of input neurons and the number of columns is the number of output neurons. Therefore our matrix of weights should be of shape (2 * 3) (that means for each input we have 3 outputs) for hidden layer. Then we are going to perform a dot product between these 2 matrices and the resultant matris will have the shape of (200 * 3)\n",
    "\n",
    "Now, Let's move on to the next layer.\n",
    "\n",
    "![NN Architecture3](NN_from_scratch_architecture_pic3.png)\n",
    "\n",
    " So the (200 * 3) matrix that we have got for hidden layer becomes inputs to the next layer. And for weights, we have 3 neurons in the inputs and 2 neurons in the output. So our weight matrix is (3 * 2). So the outputs that we will get from output layer is going to be (200 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation function and weight retrival function\n",
    "# at first, we're going to represent the model as a dictionary\n",
    "#model_dict = {'w1' : w1, 'b1' : b1, 'w2' : w2, 'b2' : b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive(model_dict):\n",
    "    #retreives weights from model dictionary\n",
    "    w1 = model_dict[\"w1\"]\n",
    "    b1 = model_dict['b1']\n",
    "    w2 = model_dict[\"w2\"]\n",
    "    b2 = model_dict['b2']\n",
    "    \n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4516cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation function\n",
    "def forward(x, model_dict):\n",
    "    w1, b1, w2, b2 = retreive(model_dict)\n",
    "    z1 = x.dot(w1) + b1 #net input value of first neuron of hidden layer\n",
    "    a1 = np.tanh(z1)  #activation function\n",
    "    z2 = a1.dot(w2) + b2 #net input value of first neuron of output layer\n",
    "    a2 = np.tanh(z2)\n",
    "    #now we need to calculate softmax and then we can easily draw and inference by\n",
    "    #taking the maximum as our prediction\n",
    "    exp_scores = np.exp(a2)\n",
    "    softmax = exp_scores / np.sum(exp_scores, axis = 1, keepdims = True) #dim=1, because across axis 0 (row) they are smples and across axis 1 (columns) is the output of output neurons\n",
    "    return z1, a1, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb3f38",
   "metadata": {},
   "source": [
    "Here, we have a softmax output with a shape of (200, 2), meaning we have 200 rows, and each row represents the probability distribution for two possible classes or outcomes (e.g., class 0 and class 1).\n",
    "\n",
    "To determine which class was correctly predicted for each row, we look at the corresponding value in the true labels vector, often denoted as \"y.\" If, for instance, the 5th value in the \"y\" vector is 1, it means that the true class for that particular example is class 1. Conversely, if it's 0, it means the true class is class 0.\n",
    "\n",
    "In this context, when calculating the cross-entropy loss, we only consider the loss for the correct predictions. So, if the 5th value in \"y\" is 1 (indicating the true class is class 1), we would look at the 5th row in the softmax output and calculate the loss based on the probability assigned to class 1 in that row. Conversely, if the 5th value in \"y\" is 0 (indicating the true class is class 0), we would calculate the loss based on the probability assigned to class 0 in the 5th row of the softmax output.\n",
    "\n",
    "In summary, the true class labels in \"y\" help us identify which class was actually correct for each example, and we use this information to compute the cross-entropy loss only for the correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625cc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss calculation (cross entropy loss)\n",
    "def loss(softmax, y, model_dict):  #softmax=predicted labels, y=actual labels\n",
    "    #firstly retreive the wights as we'll apply L2 regularization technique\n",
    "    w1, b1, w2, b2 = retreive(model_dict)\n",
    "    \n",
    "    m = np.zeros(200)\n",
    "    for i,correct_index in enumerate(y):\n",
    "        predicted = softmax[i][correct_index]\n",
    "        m[i] = predicted\n",
    "    \n",
    "    log_prob = -np.log(predicted)\n",
    "    softmax_loss = np.sum(log_prob)\n",
    "    reg_loss = lamb / 2 *(np.sum(np.square(w1)) + np.sum(np.square(w2)))\n",
    "    loss = softmax_loss + reg_loss \n",
    "    return float(loss / y.shape[0])  #to normalize by the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7125c7",
   "metadata": {},
   "source": [
    "To gain a deeper understanding of how the softmax equation within the 'forward()' function operates and how the 'for' loop within the 'loss()' function systematically extracts predicted values for the expected class, please refer to the notebook. [click here](https://github.com/Ifthekher237/DeepLearning-Projects/blob/main/Understanding%20Softmax%20and%20Loss%20Calculation%20in%20Neural%20Networks.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a2517",
   "metadata": {},
   "source": [
    "So before we write the function for the backward propagation, we're going to write the function for the prediction. And this function, we're going to use it in the testing time. So during inference, we're going to use this predict function to predict the output.\n",
    "\n",
    "We actually do it by taking the maximum output of softmax function. So maximum output is the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57672f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just copy pased the same function that was written for forward propagation function\n",
    "# and made changes in 2 places: function name and the return value of the function \n",
    "def predict(x, model_dict):\n",
    "    w1, b1, w2, b2 = retreive(model_dict)\n",
    "    z1 = x.dot(w1) + b1 #net input value of first neuron of hidden layer\n",
    "    a1 = np.tanh(z1)  #activation function\n",
    "    z2 = az.dot(w2) + b2 #net input value of first neuron of output layer\n",
    "    a2 = np.tanh(z2)\n",
    "    #now we need to calculate softmax and then we can easily draw and inference by\n",
    "    #taking the maximum as our prediction\n",
    "    exp_scores = np.exp(a2)\n",
    "    softmax = exp_scores / np.sum(exp_scores, dim = 1, keepdims = True) #dim=1, because across axis 0 (row) they are smples and across axis 1 (columns) is the output of output neurons\n",
    "    return np.argmax(softmax, axis=1)  #returns the idex number where maximum occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffe28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(x, y, model_dict, epochs):\n",
    "    for i in range(epochs):\n",
    "        w1, b1, w2, b2 = retreive(model_dict)\n",
    "        z1, a1, probs = forward(x, model_dict)\n",
    "        \n",
    "        #start applying backpropagation\n",
    "        delta3 = np.copy(probs)\n",
    "        #delta3[range(x.shape[0]), y] #it extracts out the predicted values of the expected class\n",
    "        delta3[range(x.shape[0]), y] -= 1  # (200 by 2) delta3 = probs - 1\n",
    "        dw2 = (a1.T).dot(delta3)   #gradient of loss w.r.t w2, a1:(200,3), delta=(200,2) --> (3,2)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims = True) # (1,2), bias for each neuron of output layer\n",
    "        delta2 = delta3.dot(w2.T) * (1-np.power(np.tanh(z1),2))\n",
    "        dw1 = np.dot(x.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        #add regularization terms\n",
    "        dw2 += lamb * np.sum(w2)  #see the image attached below to understand why.\n",
    "        dw1 += lamb * np.sum(w1)\n",
    "        \n",
    "        #update the weights (W <== W + (-lr*gradient))\n",
    "        w1 += -learning_rate * dw1       \n",
    "        b1 += -learning_rate * db1\n",
    "        w2 += -learning_rate * dw2\n",
    "        b2 += -learning_rate * db2\n",
    "        \n",
    "        #update the model dictionary\n",
    "        model_dict = {'w1' : w1, 'b1' : b1, 'w2' : w2, 'b2' : b2}\n",
    "        \n",
    "        #print the loss(will be printing the loss in every 5o epochs)\n",
    "        if i%50 == 0:\n",
    "            print(\"loss at epoch {} is : {:.3f}\".format(i, loss(probs, y, model_dict)))\n",
    "            \n",
    "    return model_dict  #updated model_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d2b21",
   "metadata": {},
   "source": [
    "![regularization](NN_from_scratch_architecture_pic4_regularization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e92d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights(Xavier initialization)\n",
    "def init_network(input_dim, hidden_dim, output_dim):\n",
    "    model = {}\n",
    "    w1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "    #initially biases aren't initialized with normal dist but as zeros. Later it's gonna be updated\n",
    "    b1= np.zeros((1, hidden_dim))  #because we have 1 bias for each neuron of hidden layer\n",
    "    w2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "    b2 = np.zeros ((1, output_dim))\n",
    "    \n",
    "    model['w1'] = w1\n",
    "    model['b1'] = b1\n",
    "    model['w2'] = w2\n",
    "    model['b2'] = b2\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde978d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dict = init_network(input_dim = input_neurons, hidden_dim = 3, output_dim = output_neurons)\n",
    "model = backpropagation(x, y, model_dict, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f749d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76981c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712a701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
