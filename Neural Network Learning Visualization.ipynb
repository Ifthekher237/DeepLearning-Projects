{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18879237",
   "metadata": {},
   "source": [
    "In this notebook, we're going to actually visualize the learning process of neural networks. We're going going to see a step by step visualization on how neural networks are learning. And in this notebook, we're also going to actually understand the essence of neural networks and how good they are at separating non-linear data(using non-linear activation functions and multiple layers of neurons.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfad9a5",
   "metadata": {},
   "source": [
    "So the package ***torch.nn.functional*** is going to allow us to use the Relu activation function, which is the non-linearity.\n",
    "\n",
    "***sklearn.datasets*** to load a ready made dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed34278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now importing the dataset\n",
    "x,y = sklearn.datasets.make_moons(200, noise=.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70985f70",
   "metadata": {},
   "source": [
    "***sklearn.datasets.make_moons(200, noise=0.20)*** is using the **make_moons** function from the scikit-learn library to generate a synthetic dataset(A synthetic dataset is a dataset that is artificially generated rather than being collected from real-world observations or measurements) with two classes (binary classification) that resembles two crescent-shaped moon patterns. \n",
    "\n",
    "**200** is the number of samples or data points to generate.\n",
    "\n",
    "**noise=0.20** specifies the amount of random noise to be added to the data.\n",
    "\n",
    "**x** = input = feature matrix;\n",
    "\n",
    "**y** = expected output = target labels or classes associated with each data point. In this binary classification scenario, y will contain labels that indicate which class (0 or 1) each data point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9433e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2306a4",
   "metadata": {},
   "source": [
    "**x** has two columns, each representing a different feature. These features  represents the coordinates (x and y positions) of each data point on a two-dimensional plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93291ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630f5a6",
   "metadata": {},
   "source": [
    "since this is a binary classification problem. **y** has an array of 0 or 1. That means, each 0 or 1 indicate each point on the plane belong to either class 0 or class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd1a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's plot x\n",
    "plt.scatter(x[:,0], x[:,1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b4e8f",
   "metadata": {},
   "source": [
    "As we see the 2 classes can't be seperated by a straight line. So non-linearity is introduced here. And we call them non-linearly seperable. That's where we need to use more complex algorithm rather than just linear or logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a324c",
   "metadata": {},
   "source": [
    "**x[:,0]** and **x[:,1]** are used for the x and y coordinates of the data points in the plot.\n",
    "\n",
    "**s=40** sets the size of the data points to be relatively large (size 40).\n",
    "\n",
    "**c=y** assigns colors to the data points based on the values in the y variable. Each class (0 or 1) is given a different color.\n",
    "\n",
    "**cmap=plt.cm.Spectral** specifies the color scheme for the plot, with distinct colors for each class.\n",
    "\n",
    "The resulting plot visually displays data points on a graph, where each point's position is determined by its features (x and y values), and the colors represent which class (0 or 1) each data point belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into torch tensor.\n",
    "x = torch.FloatTensor(x)\n",
    "y = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ffa52",
   "metadata": {},
   "source": [
    "**FloatTensor**: Use this when your data contains decimal numbers or real values. It's suitable for tasks involving measurements, sensor readings, or any data with decimal points.\n",
    "\n",
    "**LongTensor**: Use this when your data contains whole numbers, typically for tasks like classification where you have discrete class labels represented as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f411f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbac0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96faf0ed",
   "metadata": {},
   "source": [
    "Now we're going to build and structure our neural network(Input layer, 1 hidden layer and output layer). And to do that we're going to build a class and inside this class, we're going to structure the neural network as well as apply the forward propagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0960c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons):  #since we have 3 layers\n",
    "        super(FeedForward, self).__init__()\n",
    "        #we're going to start building our layers.\n",
    "        self.hidden = nn.Linear(input_neurons, hidden_neurons) #hiddenlayer\n",
    "        self.output = nn.Linear(hidden_neurons, output_neurons)  #outputlayer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #the forward function is already included in the module \n",
    "        #but we are overriding it to suit our current NN.\n",
    "        out = self.hidden(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978d916",
   "metadata": {},
   "source": [
    "We've defined our class that defines our neural network, let's create an object of that network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = FeedForward(input_neurons = 2, hidden_neurons = 50, output_neurons = 2)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr = 0.02)\n",
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d40d6",
   "metadata": {},
   "source": [
    "since, we have 2 columns only in x, that means 2 input features. And for this NN we take 50 neurons in hidden layers and since it's a binary classification so there are 2 output neurons.\n",
    "\n",
    "To use BCELoss, we have to have 1 output neurons. Moreover, it doesn't use the Softmax function. But here, we have 2 output neurons. So we use Cross Entropy which uses softmax function. In PyTorch we don't have to define the softmax function, PyTorch automatically defines it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90128155",
   "metadata": {},
   "source": [
    "Now we're going to train our network. And while we're training it, we're going to visualize the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b62dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion() #interactive mode on to plot the points on the same plot\n",
    "for epoch in range(10000):\n",
    "    #forward propagating x by calling the object that we created\n",
    "    out = network(x)\n",
    "    \n",
    "    #calculating the loss\n",
    "    loss = loss_function(out, y) #predicted=out,actual=y\n",
    "    \n",
    "    #backpropagation\n",
    "    #clearing gradient buffer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #calculate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    #visualization\n",
    "    #we want to plot how much our NN progressed after every 1000 epochs\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        # show the learning process up untill now\n",
    "        # taking the maximum of our output predictions. \n",
    "        # We don't need to calculate it manually since softmax is blended by default in the Cross entropy calculation\n",
    "        max_value, prediction = torch.max(out,1) #since softmax is distributed across the columns so axis=1, prediction=index of that maximum value\n",
    "        predicted_y = prediction.data.numpy() #converting torch tensor to numpy array as matplotlib expect a numpy array\n",
    "        target_y = y.data.numpy() #converting the actual y values to numpy array\n",
    "        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:,1], s=40, c=predicted_y, lw=0 ) #lw=line width\n",
    "        \n",
    "        #calculating the accuracy \n",
    "        accuracy = (predicted_y == target_y).sum() / target_y.size  #normalizing by dividing because we want the accuracy to be in terms of 0 to 1 and multiply it by 100 to get the percentage\n",
    "        plt.text(3,-1, 'Accuracy = {:.2f}'.format(accuracy), fontdict = {'size':14})\n",
    "        plt.pause(0.1)\n",
    "        \n",
    "        \n",
    "plt.ioff()        \n",
    "plt.show\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afd1ef",
   "metadata": {},
   "source": [
    "So as we can see, the neural network was able to classify these two data very accurately though they are non-linear because they can't be separated by a line. They need to be separated by a curve.\n",
    "\n",
    "So that is how good our neural networks to separate or classify nonlinear data very."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9addc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae6b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980f83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ecad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa33270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e023dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aadfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089b7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
