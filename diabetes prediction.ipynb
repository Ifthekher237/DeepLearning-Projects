{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5213e3f",
   "metadata": {},
   "source": [
    "**Pandas** and **Sckikit_learn** libraries will help us to preprocess our dataset, and after everything is ready, we are going to build our PyTorch custome dataset using **torch.utilis.data**\n",
    "\n",
    "**Pandas:** we will use Pandas to load, manipulate, and preprocess our raw dataset. Pandas is a powerful Python library for data analysis that allows us to perform various data cleaning, transformation, and exploration tasks.\n",
    "\n",
    "**Scikit-learn (sklearn):** Scikit-learn will help us with additional data preprocessing tasks like handling missing values, feature scaling, encoding categorical variables, and splitting the data into training and testing sets. It provides a range of tools to prepare our data for machine learning.\n",
    "\n",
    "**torch.utils.data:**  module to create a custom dataset that can be directly fed into our PyTorch neural network for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66dae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load the dataset using pandas\n",
    "#this dataset has 700+ samples and each sample has 7 features and 1 output.\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66344ce",
   "metadata": {},
   "source": [
    "Now, we need to pre-process the dataset, first of all we need to extract our X and Y. X = input = all the features except \"Class\" which is output class.\n",
    "Y = output = only the \"class\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For x: Extract out the dataset from all the rows (all samples) and all columns except last column (all features)\n",
    "# For y: Extract out the last column (which is the label)\n",
    "# Convert both to numpy using the .values method\n",
    "x = data.iloc[:,:7].values\n",
    "y_string = list(data.iloc[:,-1]) #since our output classes are string, so first store the string then convert it in numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2035d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our neural network only understands numbers! So convert the string to labels(numbers)\n",
    "y_int = []\n",
    "for string in y_string:\n",
    "    if string == 'positive':\n",
    "        y_int.append(1)\n",
    "    else:\n",
    "        y_int.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ddec63",
   "metadata": {},
   "source": [
    "Now, convert the **y_int** list to an array. Because we are doing a neural netwrok and everything has to be in array or matrix format since we're using NumPy. We use NumPy because Python lists are slow and we can't basically do some matrix operations on them. That's why we need to convert everything to a numpy array and after that we need to convert it also to a pytorch tensor. We can actually convert a list directly to PyTorch tensor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e98a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert to an array\n",
    "y = np.array(y_int, dtype = 'float64')\n",
    "# since x has float numbers, so we specify y to be float as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70141e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1030cb",
   "metadata": {},
   "source": [
    "Each feature has different range. So we need to normalize the data. Normalization is needed in deep learning to ensure that input features are on a consistent scale. It prevents certain features from dominating others during training, which can help the model converge faster and improve its ability to generalize to new data. Normalization typically involves scaling features to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "There are many other reasons why Normalization is needed.\n",
    "- **Stabilizes Training:** It ensures that when the model learns, it doesn't get confused by the differences in how big or small the data is.\n",
    "- **Accelerates Convergence:** Speeds up learning by ensuring balanced gradients.\n",
    "- **Improves Generalization:** Enhances model's ability to make accurate predictions on unseen data.\n",
    "- **Enhances Gradient Descent:** Facilitates more efficient gradient descent optimization.\n",
    "- **Facilitates Weight Initialization:** Helps weight initialization methods work effectively.\n",
    "- **Mitigates Overfitting:** Acts as implicit regularization by reducing the impact of outliers.\n",
    "- **Ensures Model Robustness:** Makes the model less sensitive to variations in input scale.\n",
    "- **Compatibility with Activation Functions:** Ensures activations stay within desired ranges.\n",
    "- **Interpretability:** Enables better understanding of learned features and model behavior.\n",
    "- **Prevents Numerical Instabilities:** Guards against numerical issues in computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032fa87",
   "metadata": {},
   "source": [
    " $x' = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "This is the formula that we're going to apply for Normalization. This is caled Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b769a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Normalization. All features should have the same range.\n",
    "# We're going to do this using scikit learn library\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278e31a",
   "metadata": {},
   "source": [
    "What we did in the previous cell is that:\n",
    "\n",
    "we create an instance of the StandardScaler class, such as sc, to act as our normalization object. Once we have this object, we can use its **fit_transform** method to apply the normalization to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d903e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19612422",
   "metadata": {},
   "source": [
    "There are other Normalization techniques as well, such as, Mean Normalization, Min-Max scaling etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we convert the arrays to PyTorch tensors.\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape\n",
    "#768 samples and 7 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d24b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbea519",
   "metadata": {},
   "source": [
    "Since, we'll be using binary cross entropy as our loss function, so target variable(y) must be 2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dae635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20767829",
   "metadata": {},
   "source": [
    "Let's continue to build our custome PyTorch dataset. Firstly, we are going to create a class named \"Dataset\" and we are going to inherit this class from the ***Dataset*** (which is the base class for creating custom datasets in PyTorch) class that we imported from ***torch.utils.data***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self,index):        \n",
    "       # actually we're overriding the getitem function from the inherited class Dataset.\n",
    "        #Because we're building a custom dataset and we want our function to perform differently.\n",
    "        #And this function is in charge of getting one sample of the datasets.\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        #this is also a overridden function\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4093315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(x,y) # done creating our object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a05975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to our dataloader for batch processing and shuffling\n",
    "train_loader = DataLoader(dataset= dataset,\n",
    "          batch_size = 32,\n",
    "          shuffle = True)\n",
    "# shuffle=True mixes up the data for training, while \n",
    "#shuffle=False keeps it in order for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080825d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the data loader\n",
    "print(\"There is {} batches in the dataset\".format(len(train_loader)))\n",
    "for (x,y) in train_loader:\n",
    "    print(\"For one iteration (batch), there is:\")\n",
    "    print(\"Data:   {}\".format(x.shape))\n",
    "    print(\"Labels: {}\".format(y.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c25d0b",
   "metadata": {},
   "source": [
    "The ***len(train_loader)*** gives you the number of mini-batches that the dataset has been divided into.\n",
    "\n",
    "The ***for*** loop iterates over the mini-batches in *train_loader*. It doesn't go through the data that inside each mini-batches.\n",
    "\n",
    "The ***x.shape*** gives you the dimensions of the data, which is a tuple representing the batch size and the input feature dimensions for that mini-batch.\n",
    "\n",
    "***y.shape*** gives you the dimensions of the labels, which typically correspond to the batch size and the number of classes or regression targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c12d4",
   "metadata": {},
   "source": [
    "![my image](diabetes_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aed310",
   "metadata": {},
   "source": [
    "It's upto us, whether we want to use 3 hidden layer or 2 hidden layer or only 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de09447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's build the above network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,input_feauters, output_features):\n",
    "        super(Model, self).__init__()\n",
    "        #now we're defining the attributes of our NN\n",
    "        self.fc1 = nn.Linear(input_feauters, 5) #for the 1st layer, output features=5\n",
    "        self.fc2 = nn.Linear(5,4)\n",
    "        self.fc3 = nn.Linear(4,3)\n",
    "        self.fc4 = nn.Linear(3,output_features)\n",
    "        \n",
    "        #for hidden layers we'll use Tanh and for output layer we'll use Sigmoid(because we used BCE loss) activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    #defining the functionalities\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3455b",
   "metadata": {},
   "source": [
    "We're done building our neural network now. We don't need to build or code our backpropagation function because PyTorch will automatically do it for us. All we need to do is supply the forward propagation function and pytorch automatically does the back propagation.\n",
    "\n",
    "fc = fully connected layer/ linear layer / multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7a029",
   "metadata": {},
   "source": [
    "In a neural network, there are ***attributes*** and ***functionalities*** that define its architecture and behavior. Here are common attributes and functionalities associated with a neural network:\n",
    "\n",
    "## Attributes\n",
    "\n",
    "  **Layers:** A neural network typically consists of multiple layers, including input, hidden, and output layers. Each layer is an attribute of the neural network.\n",
    "\n",
    "  **Weights and Biases:** Neural networks learn from data by adjusting weights and biases associated with each connection between neurons. These weights and biases are learned during training and are crucial attributes of the network.\n",
    "\n",
    "  **Activation Functions:** The type of activation functions used in each layer (e.g., ReLU, Sigmoid, Tanh) is an attribute that defines how the neurons in that layer process input.\n",
    "\n",
    "  **Loss Function:** The loss function used for training, which measures the error between predicted and actual outputs, is an attribute.\n",
    "\n",
    "  **Optimization Algorithm:** The optimization algorithm used for updating weights and biases during training (e.g., SGD, Adam) is another attribute.\n",
    "\n",
    "  **Learning Rate:** The learning rate, which determines the step size for weight updates during training, is often an attribute that can be adjusted.\n",
    "\n",
    "## Functionalities:\n",
    "\n",
    "  **Forward Propagation:** The neural network should have a functionality to perform forward propagation, which involves passing input data through the network to make predictions.\n",
    "\n",
    "  **Backpropagation:** During training, the network should be able to compute gradients and update weights and biases using backpropagation, a core functionality for learning from data.\n",
    "\n",
    "  **Inference:** After training, the network should be capable of making predictions (inference) on new, unseen data.\n",
    "\n",
    "  **Regularization:** The network can implement regularization techniques (e.g., dropout, L1/L2 regularization) to prevent overfitting.\n",
    "\n",
    "  **Evaluation:** It should provide functionality for evaluating its performance on validation or test datasets using metrics such as accuracy, precision, recall, etc.\n",
    "\n",
    "  **Saving and Loading:** Often, neural networks need to save their learned weights and architecture to disk and load them for future use.\n",
    "\n",
    "  **Hyperparameter Tuning:** Some neural networks may have functionality to search for optimal hyperparameters (e.g., learning rate, batch size) automatically.\n",
    "\n",
    "  **Visualization:** Tools for visualizing model architecture, training curves, and feature maps can be helpful for debugging and analysis.\n",
    "\n",
    "The specific attributes and functionalities of a neural network can vary depending on the type of network (e.g., feedforward, convolutional, recurrent), the problem it is designed to solve (e.g., classification, regression, generation), and the framework or library used for implementation (e.g., PyTorch, TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc791db",
   "metadata": {},
   "source": [
    "$ H_{p}(q) = \\frac{-1}{N} \\sum_{i=1}^{N} y_{i} . \\log{p(y_{i})} + (1 - y_{i}).\\log{1 - p(y_{i})} $\n",
    "\n",
    "cost = -(Y  torch.log(hypothesis) + (1 - Y)torch.log(1 - hypothesis)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, create the network(an object of the Model class)\n",
    "net = Model(7,1)\n",
    "# Since our output is either 0 or 1. So we use BCE loss function\n",
    "# In BCE loss function: the input and output should have the same shape\n",
    "# size_average = True --> the losses are averaged over observations for each minimatch\n",
    "criterion = torch.nn.BCELoss(size_average = True)\n",
    "\n",
    "# Finally our optimaztion  algorithm!\n",
    "# we will use SGD with momentum with a learning rate of 0.1\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.1, momentum =0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4731031",
   "metadata": {},
   "source": [
    "If ***size_average*** is set to **True**, it means that the loss value will be an average of the errors for each item in the batch. This is useful when we want the loss to be roughly the same scale, regardless of the batch size.\n",
    "\n",
    "If ***size_average*** is set to **False**, it means that the loss value will be the sum of the errors for each item in the batch. This can be useful when we want to know the total error for the entire batch without considering the batch size.\n",
    "\n",
    "The choice between **True** and **False** depends on what you need for our specific problem and how we want to interpret the loss value.\n",
    "\n",
    "***net.parameters()*** specifies the parameters (weights and biases) that the optimizer will update during training. We know here **net** is an instance of our neural network model, and ***net.parameters()*** retrieves all the learnable parameters from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network\n",
    "# let's train our hyperparameters\n",
    "epochs = 200\n",
    "for epoch in range(200):\n",
    "    for inputs,labels in train_loader:\n",
    "        inputs = inputs.float() #though everyting is in float dtype, just to be sate\n",
    "        labels = labels.float()\n",
    "        #let's feed our data to the the NN\n",
    "        #Forward prop\n",
    "        outputs = net(inputs)\n",
    "        # loss calculation\n",
    "        loss = criterion(outputs, labels) #predicted val=outputs, Actual val=labels\n",
    "        \n",
    "        # let's go ahead and begin back prop. There are 3 steps for the FP in PyTorch\n",
    "        \n",
    "        # firstly, clear the gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #secondly, calculate the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        #Thirdly, update the weights\n",
    "        optimizer.step() # new_weight <-- old_weight - lr*gradient\n",
    "        \n",
    "        \n",
    "    # Now we want to calculate the training accuracy after each epoch.\n",
    "    output = (outputs>0.5).float()  # 0.5 is a threshold value.\n",
    "    accuracy = (output == labels).float().mean()    \n",
    "    # Print statistics\n",
    "    print(\"Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}\".format(epoch+1, epochs, loss, accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a488773",
   "metadata": {},
   "source": [
    "There are 3 steps for the forward propagation in PyTorch. \n",
    "### Clearing the Gradient Buffer:\n",
    "Firstly, clear the gradient buffer. This is not to accumulate gradients.If gradients are not cleared between batches, they would accumulate over time. This means that the gradients from the current batch would be added to the gradients from previous batches. As a result, the parameter updates in subsequent batches would be influenced by the cumulative gradients, potentially leading to incorrect updates and unstable training. To avoid gradient accumulation, it is a common practice to clear the gradient buffer (i.e., set all gradients to zero) at the beginning of each batch. This ensures that the gradients computed for the current batch are independent of any previous batches. It's typically done using a command like **optimizer.zero_grad()** in PyTorch.\n",
    "### Calculating the Gradient (Backward Pass):\n",
    "The second step we need to calculate the gradient.So we're not going to apply the the weight update rule yet. All we're doing right now is just calculating the gradients. We do this by **loss.backward()**. Now when you call **loss.backward()**, this will perform the the back propagation and calculate all the gredients. so you'll have a matrix of gradients with respect to each model parameter. So this is also the back propagation.\n",
    "\n",
    "### Updating the Weights (Optimization Step):\n",
    "Updates the model's weights based on the computed gradients. This step applies the optimization algorithm (e.g., stochastic gradient descent) to make small adjustments to the weights in the direction that reduces the loss. **optimizer.step()** is used to apply the weight update rule to all model parameters. It internally uses the gradients computed in the backward pass to update the weights.\n",
    "\n",
    "\n",
    "## Training Accuracy Calculation:\n",
    "\n",
    "**output = (outputs > 0.5).float()**: Converts the model's output to binary values (0 or 1) by comparing them to a threshold (0.5).\n",
    "\n",
    "**accuracy = (output == labels).float().mean()**: Calculates the training accuracy by comparing the binary predictions (**output**) to the actual labels (**labels**) and computing the mean accuracy for the current mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee5383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9addcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fff565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
