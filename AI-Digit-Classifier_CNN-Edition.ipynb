{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8e959d",
   "metadata": {},
   "source": [
    "In this notebook, we're going to code a CNN. A convolutional neural network to classify handwritten digits. In another project, we coded up a feedforward neural network to classify handwritten digits.There, we flattened the image out all of the pixels. So we had 28 by 28 pixels flattened out, and each pixel was a feature. And then we had some hidden layers. We built a neural network with three hidden layers to classify this handwritten digit.\n",
    "\n",
    "This is the CNN that we are going to built\n",
    "![cnn_architecture](image_classification_architecture.png)\n",
    "\n",
    "#### Here's a concise explanation for each step:\n",
    "\n",
    "- **Input:** The network takes as input grayscale images of handwritten digits (0-9) from the MNIST dataset.\n",
    "\n",
    "- **First Convolutional Layer:** This initial layer consists of eight filters that scan the input image to detect simple patterns, like edges and corners.\n",
    "\n",
    "- **Max Pooling:** After the first convolution, max-pooling is applied to reduce the dimensions of the feature maps while retaining essential information.\n",
    "\n",
    "- **Second Convolutional Layer:** A more complex layer with 32 filters is employed to capture higher-level features, like shapes and textures.\n",
    "\n",
    "- **Max Pooling:** Similar to the first pooling layer, this reduces dimensions further and keeps critical features.\n",
    "\n",
    "- **First Fully Connected Layer:** This layer flattens the max-pooled feature maps and connects them to 600 neurons, allowing for more advanced feature extraction.\n",
    "\n",
    "- **Second Fully Connected Layer (Classification):** The final layer consists of 10 neurons, each corresponding to a digit (0-9). It classifies the input image into one of these ten categories, completing the digit recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9cd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333d928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameters for data normalization (mean & SD), these are pre-calculated for MNIST dataset\n",
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "# input[channel] = (input[channel]-mean[channel]) / std[channel] <== formula of data normalization\n",
    "transforms_original = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean_gray,), (stddev_gray))])\n",
    "                                \n",
    "# later we'll define another transformation \"transforms_photo\" to detech our handwritten image.\n",
    "    \n",
    "train_dataset = datasets.MNIST(root='./',\n",
    "                              train=True,\n",
    "                              transform=transforms_original,\n",
    "                              download=True)\n",
    "                                \n",
    "\n",
    "test_dataset = datasets.MNIST(root='./',\n",
    "                              train=False,\n",
    "                              transform=transforms_original,\n",
    "                              download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afcc16a",
   "metadata": {},
   "source": [
    "***transforms.Compose()*** is a utility function provided by PyTorch that allows us to create a sequence of image transformations to be applied to our data in a specific order. It takes a list of individual transformations as input and returns a single composed transformation. Here, we've defined a list of individual transformation functions: ***transforms.ToTensor()*** and ***transforms.Normalize()***, where, ***transforms.ToTensor()*** converts the image data, which is originally in the form of pixels, into PyTorch tensors. ***transforms.Normalize((mean_gray,), (stddev_gray))*** normalizes the tensor values of the image.  Data normalization makes sure that the data has a mean of zero and a standard deviation of one, which can help with training stability and convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8cf3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "random_img = train_dataset[20][0].numpy() * stddev_gray + mean_gray  \n",
    "\n",
    "plt.imshow(random_img.reshape(28,28), cmap = 'gray')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# to print image label\n",
    "print(train_dataset[20][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ca690",
   "metadata": {},
   "source": [
    "Each image has 2 input, first one is the image itself and the 2nd one is the label of the image, that is, the number that is written in the image. Here, we want to look at the 2oth image (index 0) but not it's label. We used **.numpy()** to convert the image to an array to plot and since each image is already normalized before, so to visualize it we need to denormalize the image again.\n",
    "\n",
    "To see the image we have to reshape it correctly, that is, to its original shape and since all MNIST images are in grayscale so we use 'gray' colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4617e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61357f5a",
   "metadata": {},
   "source": [
    "That line is simply **resetting Matplotlib’s styling and configuration back to its factory defaults**. Here’s what’s happening:\n",
    "\n",
    "* **`plt.rcParams`** is a dictionary-like object that holds all of Matplotlib’s runtime settings (fonts, line widths, color cycles, figure sizes, etc.).\n",
    "* **`plt.rcParamsDefault`** is another dict that contains the original default values for all of those settings.\n",
    "* **`.update(plt.rcParamsDefault)`** replaces whatever you might have changed in `rcParams` with the defaults from `rcParamsDefault`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aeac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_load = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = True)\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9fb5cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_load)   # 6000 / 100 ==> 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1ab11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # While it's possible to specify all the parameters as input of init function, \n",
    "        # but here I'll directly hardcode all parameters inside the function\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # first Conv layer\n",
    "        # same padding --> input_size = output_size\n",
    "        # same padding = (filter_size - 1)/2 --> (3-1)/2 = 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 3, stride = 1, padding = 1)   # 1st Conv layer\n",
    "        # since same padding is used,so output size is also 28*28 (calculation shown below)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8) # since there are 8 feature maps\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # MaxPool for Conv1\n",
    "        # in this cnn, maxpooling is applied 2 times but both times same kernel size is used.\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2)\n",
    "        # the output size = 28/2 = 14\n",
    "        \n",
    "        # second Conv layer\n",
    "        # same padding --> input_size = output_size\n",
    "        # same padding = (filter_size - 1)/2 --> (5-1)/2 = 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels = 8, out_channels = 32, kernel_size = 5, stride = 1, padding = 2)\n",
    "        # output size of each of the 32 feature maps [(14 - 5 + 2*2)/1 + 1] = 14\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32) # since there are 8 feature maps\n",
    "        \n",
    "        # MaxPool for Conv2\n",
    "        # maxpooling already specified once, since same kernel_size is used so we don't need to specify it agian\n",
    "        # # the output size after maxpooling again = 14/2 = 7\n",
    "        \n",
    "        # fully connected 2 layers\n",
    "        # flaten the 32 feature maps: 7*7*32 = 1568\n",
    "        self.fc1 = nn.Linear(in_features=1568, out_features=600) # input_neurons(calculated), output_neurons(choosen)\n",
    "        # we'll use the same ReLU activation function\n",
    "        self.dropout = nn.Dropout(p = 0.5)   # Explanation given below\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=10)\n",
    "        # so DONE with initialization functions\n",
    "        \n",
    "    # Let's define forward function\n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        #Now we have to flatten the output. This is where we apply the feed forward neural network as learned before! \n",
    "        #It will take the shape (batch_size, 1568) = (100, 1568)\n",
    "        out = out.view(-1,1568) # flattening the 32 feature maps to feed it to the FC1 (100, 1568)\n",
    "        \n",
    "        # Then we forward through our fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09b9b1",
   "metadata": {},
   "source": [
    "conv2d is the class from the model which is responsible for constructing a convolutional layer.\n",
    "\n",
    "For the first convolutional layer:\n",
    "\n",
    "- Number of input channels: look at the previous layer, we see that there is a grayscale image, so the number of input channel is 1.\n",
    "- Number of output channels (filters): 8 (each filter will produce a feature map)\n",
    "- Filter size (kernel size): 3x3.\n",
    "- Stride: 1 (to avoid downsampling).\n",
    "- Padding: 1.\n",
    "- the output size of each of the 8 feature maps:\n",
    "\n",
    "$$\n",
    "\\frac{\\text{input\\_size} - \\text{filter\\_size} + 2 \\times (\\text{padding})}{\\text{stride}} + 1\n",
    "\\;=\\;\n",
    "\\frac{28 - 3 + 2 \\times 1}{1} + 1\n",
    "\\;=\\;\n",
    "28\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It's a best practice to include a Dropout layer between fully connected layers to enhance regularization. We didn't do this previously but from now on we'll continue adding it. In this case, a Dropout layer with a probability of P=0.5 is added between the fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af08b391",
   "metadata": {},
   "source": [
    "## What’s Happening in This CNN Class\n",
    "\n",
    "This `CNN` model is a small convolutional neural network for classifying 28×28 grayscale images (like MNIST digits). Here’s a step-by-step, easy-to-follow breakdown:\n",
    "\n",
    "1. **Initialization (`__init__`)**  \n",
    "   - **Conv1**: A 3×3 convolution that turns the single input channel into 8 feature maps.  \n",
    "     - We use **padding=1** so the output remains 28×28.  \n",
    "   - **BatchNorm1 + ReLU**: Normalizes each of the 8 feature maps, then applies a ReLU nonlinearity.  \n",
    "   - **MaxPool**: Halves the spatial size (28→14) by taking the maximum over each 2×2 block.  \n",
    "   - **Conv2**: A 5×5 convolution expanding from 8 to 32 feature maps, with **padding=2** to keep the size at 14×14.  \n",
    "   - **BatchNorm2 + ReLU**: Normalizes the 32 maps, then adds nonlinearity.  \n",
    "   - **MaxPool**: Again halves spatial size (14→7).  \n",
    "   - **Flatten**: Turns the 7×7×32 tensor into a single 1D vector of length 7×7×32 = 1568.  \n",
    "   - **FC1 (600 units)**: A fully connected layer learns from these 1568 features.  \n",
    "   - **Dropout (p=0.5)**: Randomly “drops” half the neurons during training to prevent overfitting.  \n",
    "   - **FC2 (10 units)**: The final layer maps to 10 outputs—one score for each digit class (0–9).\n",
    "\n",
    "2. **Forward Pass (`forward`)**  \n",
    "   - The input image goes through Conv1 → BatchNorm → ReLU → MaxPool.  \n",
    "   - Then through Conv2 → BatchNorm → ReLU → MaxPool again.  \n",
    "   - We **flatten** the pooled feature maps, run them through FC1 + ReLU + Dropout, and finally through FC2.  \n",
    "   - The raw outputs (logits) from FC2 can then be fed into a softmax or cross-entropy loss for training.\n",
    "\n",
    "### Why This Structure?\n",
    "- **Convolutions** learn to detect simple patterns (edges, corners) then more complex ones (shapes, textures).  \n",
    "- **Pooling** reduces image size, making the network faster and more robust to small shifts.  \n",
    "- **BatchNorm** and **Dropout** help stabilize and regularize training.  \n",
    "- **Fully connected layers** combine all the learned features to make the final digit prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f1f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = CNN()\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    # moving the model to gpu\n",
    "    model = model.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8873f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For one iteration, this is what happens:\n",
      "Input Shape: torch.Size([100, 1, 28, 28])\n",
      "Labels shape: torch.Size([100])\n",
      "outputs Shape torch.Size([100, 10])\n",
      "Predicted Shape: torch.Size([100])\n",
      "Predicted Tensor:\n",
      "tensor([8, 2, 8, 2, 8, 2, 2, 8, 8, 2, 2, 2, 2, 8, 2, 6, 8, 8, 8, 8, 2, 8, 8, 6,\n",
      "        2, 2, 8, 8, 2, 8, 2, 2, 4, 2, 2, 8, 8, 8, 8, 8, 8, 2, 4, 2, 8, 2, 6, 8,\n",
      "        8, 2, 3, 8, 6, 8, 2, 2, 8, 8, 2, 8, 8, 2, 9, 9, 8, 2, 2, 2, 6, 8, 8, 9,\n",
      "        8, 8, 2, 8, 8, 8, 6, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 6,\n",
      "        2, 2, 6, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# understand what is happening\n",
    "iteration = 0\n",
    "correct = 0\n",
    "for i, (inputs, labels) in enumerate(train_load):\n",
    "    # for each i we are getting 1 full batch.\n",
    "    \n",
    "    if CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    print(\"For one iteration, this is what happens:\")\n",
    "    # Each tensor propagated through the network should be 4D (batch_size, channels, rows, cols)\n",
    "    print(\"Input Shape:\", inputs.shape)\n",
    "    print(\"Labels shape:\",labels.shape)\n",
    "    output = model(inputs)\n",
    "    print(\"outputs Shape\", output.shape)\n",
    "    _, predicted = torch.max(output, 1)  # maximum across the class dimension\n",
    "    print(\"Predicted Shape:\", predicted.shape)\n",
    "    print(\"Predicted Tensor:\")\n",
    "    print(predicted)\n",
    "    correct += (predicted == labels).sum()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36acd601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.464, Training Accuracy: 87.857, Testing Loss: 0.141, Testing Accuracy: 96.200\n",
      "Epoch 2/10, Training Loss: 0.149, Training Accuracy: 95.740, Testing Loss: 0.095, Testing Accuracy: 97.160\n",
      "Epoch 3/10, Training Loss: 0.108, Training Accuracy: 96.908, Testing Loss: 0.071, Testing Accuracy: 97.820\n",
      "Epoch 4/10, Training Loss: 0.088, Training Accuracy: 97.457, Testing Loss: 0.062, Testing Accuracy: 98.080\n",
      "Epoch 5/10, Training Loss: 0.075, Training Accuracy: 97.813, Testing Loss: 0.054, Testing Accuracy: 98.310\n",
      "Epoch 6/10, Training Loss: 0.068, Training Accuracy: 98.002, Testing Loss: 0.048, Testing Accuracy: 98.490\n",
      "Epoch 7/10, Training Loss: 0.062, Training Accuracy: 98.158, Testing Loss: 0.046, Testing Accuracy: 98.550\n",
      "Epoch 8/10, Training Loss: 0.057, Training Accuracy: 98.330, Testing Loss: 0.042, Testing Accuracy: 98.610\n",
      "Epoch 9/10, Training Loss: 0.052, Training Accuracy: 98.443, Testing Loss: 0.042, Testing Accuracy: 98.580\n",
      "Epoch 10/10, Training Loss: 0.050, Training Accuracy: 98.583, Testing Loss: 0.041, Testing Accuracy: 98.750\n"
     ]
    }
   ],
   "source": [
    "# Training the cnn\n",
    "num_epochs = 10\n",
    "\n",
    "# Define the lists to store the results of Loss and Acurracy\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    # Reset these below variables to 0 athe the beginning of every epoch\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0  # loss for 1 iteration\n",
    "    \n",
    "    model.train()        # put the network in training mode\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_load):\n",
    "        if CUDA: \n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #forward prop\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        iter_loss += loss.item() #since loss is a tensor, to extract the value item, .item() method is used\n",
    "        \n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Record the correct predictions for training data and calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    train_loss.append(iter_loss/iterations) #average training loss\n",
    "    train_accuracy.append((100 * correct / len(train_dataset)))   \n",
    "  \n",
    "    \n",
    "    # testing phase\n",
    "    # for each epoch these values will reset to zero for testing phase\n",
    "    testing_loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    \n",
    "    model.eval()  # this tells pytorch that we're in testing phase\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_load):\n",
    "        \n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        testing_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    test_loss.append(testing_loss/iterations) #average training loss\n",
    "    test_accuracy.append((100 * correct / len(test_dataset)).cpu().numpy())\n",
    "    \n",
    "    print(\"Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Accuracy: {:.3f}\"\n",
    "          .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1]))       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4691f",
   "metadata": {},
   "source": [
    "since dropout and batchnorm behave differently in training and testing time so we need to define the mode whether it's training or testing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835eb29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the loss\n",
    "f = plt.figure(figsize = (10,10))\n",
    "plt.plot(train_loss, label = \"Training Loss\")\n",
    "plt.plot(test_loss, label = \"Testing Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf553bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.14061046832241117, 0.09530659989919514, 0.07135234406217933]\n",
      "\n",
      "Train Loss: [0.46378615193068984, 0.14903729490935802, 0.1075982066988945]\n",
      "\n",
      "Test Accuracy: [array(96.2, dtype=float32), array(97.159996, dtype=float32), array(97.82, dtype=float32)]\n",
      "\n",
      "Train Accuracy: [tensor(87.8567, device='cuda:0'), tensor(95.7400, device='cuda:0'), tensor(96.9083, device='cuda:0')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss: {}\\n'.format(test_loss[:3]))\n",
    "print('Train Loss: {}\\n'.format(train_loss[:3]))\n",
    "print('Test Accuracy: {}\\n'.format(test_accuracy[:3]))\n",
    "print('Train Accuracy: {}\\n'.format(train_accuracy[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa27f5",
   "metadata": {},
   "source": [
    "To extract only the numbers from test_accuracy (lists of numpy array) and train_accuracy (lists of pytorch tensors) ***.item()*** method is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbd480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_accuracy is a list of PyTorch tensors on the GPU\n",
    "# Convert the GPU tensors to CPU and extract their values as NumPy arrays\n",
    "#train_accuracy_cpu = [acc.cpu().detach().numpy() for acc in train_accuracy]\n",
    "train_accuracy = [acc.item() for acc in train_accuracy]\n",
    "test_accuracy = [acc.item() for acc in test_accuracy]\n",
    "\n",
    "# plotting the accuracy\n",
    "f = plt.figure(figsize = (10,10))\n",
    "plt.plot(train_accuracy, label = \"Training Accuracy\")\n",
    "plt.plot(test_accuracy, label = \"Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808830c",
   "metadata": {},
   "source": [
    "Now, after we've trained and tested our model, what we want to do is take any test image and see what's the prediction.\n",
    "\n",
    "Let's load an image from the testing dataset, specifically image number 30, and prepare it for the model's forward propagation. Each image in the dataset has two indices, with the first one (index 0) corresponding to the image itself. To make it compatible for the model, we'll resize the image to a four-dimensional tensor with a batch size of 1 (since we're testing one image), one channel, and dimensions of 28x28. This format is required for PyTorch's forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddc72f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : 3\n",
      "Actual is : 3\n"
     ]
    }
   ],
   "source": [
    "img = test_dataset[30][0].resize_((1,1,28,28)) #taking a random image from the dataset\n",
    "label = test_dataset[30][1] #taking the label of the img\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if CUDA:\n",
    "    model = model.cuda()\n",
    "    img = img.cuda()\n",
    "    \n",
    "outputs = model(img)\n",
    "_, predicted = torch.max(outputs,1)\n",
    "print(\"Prediction is : {}\".format(predicted.item()))\n",
    "print(\"Actual is : {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878a511",
   "metadata": {},
   "source": [
    "### Classifying own handwritten images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e128c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first we need to import 2 extra libraries to process image\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d3d04",
   "metadata": {},
   "source": [
    "now we need to make some transformation as we did earlier while loading training and testing images. Resizing, converting to tensor, normalizing and makeing it a grayscale image(will do it by cv2, but we could do it by transorms.grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d29d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_photo = transforms.Compose([transforms.Resize((28,28)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((mean_gray,),(stddev_gray))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c6c6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_name,model):\n",
    "    image = cv2.imread(img_name,0) # read the image and 0 means to transform it to grayscale image\n",
    "    \n",
    "    ret, thresholded = cv2.threshold(image,127,255,cv2.THRESH_BINARY) #explanation given below\n",
    "    img = 255-thresholded # explanation given below\n",
    "    \n",
    "    cv2.imshow('Thresholded',thresholded)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    cv2.imshow('Original',img)    # to display the image\n",
    "    cv2.waitKey(0)             # explanation given below\n",
    "    cv2.destroyAllWindows()       # explanation given below\n",
    "    img = Image.fromarray(img)    # explanation given below\n",
    "    img = transforms_photo(img)   # explanation given below\n",
    "    img = img.view(1,1,28,28)     # resize the image to the expected size(batch_size,channels,height,width) by our cnn.\n",
    "    #img = Variable(img)           # wrapping the image to a variable\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        img = img.cuda()\n",
    "        \n",
    "    output = model(img)\n",
    "    print(output)\n",
    "    print(output.data)\n",
    "    _,predicted = torch.max(output,1)\n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74f2de79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2950, -4.4155,  1.3594,  3.3704, -5.2449,  9.5330, -3.6802, -1.6542,\n",
      "          1.6370,  3.3882]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2950, -4.4155,  1.3594,  3.3704, -5.2449,  9.5330, -3.6802, -1.6542,\n",
      "          1.6370,  3.3882]], device='cuda:0')\n",
      "The predicted label is 5\n"
     ]
    }
   ],
   "source": [
    "pred = predict('5.jpeg',model)\n",
    "print(\"The predicted label is {}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0721f27",
   "metadata": {},
   "source": [
    "At first we'll make our image completely white background and black digit(foreground) so that there are no other color present in the image but since our model is trained with image of black background and white digit so we'll convert the image to such type of image.\n",
    "\n",
    "### 1. Thresholding:\n",
    "\n",
    "   - Thresholding is a technique used in image processing to create a binary image, which means that each pixel in the resulting image can be one of two values, typically 0 (black) or 255 (white). It's a way to segment an image into regions or objects based on pixel intensity.\n",
    "\n",
    "### 2. cv2.threshold Function:\n",
    "\n",
    "   - The **cv2.threshold** function in the OpenCV library is used to perform thresholding.\n",
    "   - It takes several arguments:\n",
    "        - **image**: The input image you want to threshold.\n",
    "        - **127**: The threshold value. This value is used to separate pixels into two groups: those below the threshold and those equal to or above the threshold.\n",
    "        - **255**: The maximum value assigned to pixels that pass the threshold. In this case, pixels with values greater than or equal to 127 become 255 (white).\n",
    "        - **cv2.THRESH_BINARY**: The type of thresholding. This option specifies that if a pixel value is greater than or equal to the threshold (127), it becomes the maximum value (255); otherwise, it becomes the minimum value (0, which is black).\n",
    "\n",
    "### 3. Output of Thresholding:\n",
    "\n",
    "   - After thresholding, you obtain a new image called thresholded, where pixels in regions of interest are set to 255 (white), and background pixels are set to 0 (black).\n",
    "   - Pixels below the threshold (less than 127) are set to 0, and pixels equal to or above the threshold (greater than or equal to 127) are set to 255.\n",
    "\n",
    "### 4. Creating the Image Negative:\n",
    "\n",
    "   - To invert the colors of the binary image, the code calculates the image negative.\n",
    "   - It subtracts the thresholded image from 255, effectively turning the black regions (background) to white and the white regions (digits or objects of interest) to black.\n",
    "\n",
    "### Example:\n",
    "Suppose you have a grayscale image represented as a 2D array like this:\n",
    "\\begin{bmatrix}\n",
    "  100 & 150 & 200 \\\\\n",
    "  50 & 120 & 80 \\\\\n",
    "  170 & 210 & 90 \\\\\n",
    "\\end{bmatrix}\n",
    "Using the **cv2.threshold** function with a threshold of 127 and **cv2.THRESH_BINARY**, the result would be:\n",
    "\\begin{bmatrix}\n",
    "  255 & 255 & 255 \\\\\n",
    "  0 & 255 & 0 \\\\\n",
    "  255 & 255 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "In the resulting binary image, values above the threshold (127) become 255 (white), and values below the threshold become 0 (black).\n",
    "\n",
    "Then, to create the image negative, you would subtract this image from 255, resulting in:\n",
    "\\begin{bmatrix}\n",
    "  0 & 0 & 0 \\\\\n",
    "  255 & 0 & 255 \\\\\n",
    "  0 & 0 & 255 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "**cv2.waitkey(0)**: pauses the program's execution, waiting for user input, and it will wait indefinitely until a key is pressed by the user.\n",
    "\n",
    "**cv2.destroyAllwindows()**: This function is used to close all open OpenCV windows, ensuring that any displayed windows are properly closed before the program exits.\n",
    "\n",
    "**img = Image.fromarray(img)** this line converts the image from a NumPy array (representing pixel values) into a PIL (Python Imaging Library) image (as this is the required image format for PyTorch)\n",
    "to pass it through the transformation: **transforms_photo(img)** because later we'll provide it to PyTorch model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
